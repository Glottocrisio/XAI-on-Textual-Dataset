# -*- coding: utf-8 -*-
"""XAI_review_yelp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sPvGEFJDnS7dLMo1HKxjy_hq92-LgO7y

# **XAI Project - Explaining Sentiment Analysis of Yelp Reviews**
*Cosimo Palma - Muhammad Imran - Muhammad Waheed Sabir*

Installing and importing all necessary packages for Data - Preprocessing, Training and Explanation
"""

from google.colab import drive
drive.mount('/content/drive')  #Importing "drive" to use document stored in cloud

"""Lime, Shap and Eli5 are the chosen explanations algorithms"""

!pip install eli5

!pip install lime  #XAI module

!pip install shap   #XAI module

!pip install aix360

# Commented out IPython magic to ensure Python compatibility.
import re    #regular expressions 
import string  # basic string manipulation
import csv      # basic csv manipulation
import pandas as pd #data analysis
import numpy as np #mathematical operations
import matplotlib.pyplot as plt #information visualization
import seaborn as sns #enhanced information visualization
from nltk.tokenize import word_tokenize  #NLP library
import nltk
nltk.download('punkt')
from nltk import FreqDist
# %matplotlib inline   
#With this backend, the output of plotting commands is displayed inline within frontends like the Colab notebook, 
#directly below the code cell that produced it. The resulting plots will then also be stored in the notebook document.

"""# Data Exploration

The dataset chosen for our experiment is a set of yelp reviews
"""

f= open('/content/drive/My Drive/XAI/yelp_review.csv', 'rb')
f = pd.read_csv(f) #, nrows=0).columns.tolist()
f.shape

"""With this procedure the dataset is splitted into training and testing dataset, and a new column is set: "Sentiment", a boolean semplification of the values displayed in "Label""""

# Dataset splitting into train & test subsets along with numeric class labels
with open('/content/drive/My Drive/XAI/yelp_review.csv') as csv_yelp:
    count =  0 
    #for train data
    fsc_stars= []
    fsc_review= []
    fsc_sentiment= []
    #for test data
    fsc_stars_test=[]
    fsc_review_test=[]
    fsc_sentiment_test= []
    reader = csv.DictReader(csv_yelp)
    for row in reader:
      count += 1 
      fsc_stars.append(row["stars"])
      fsc_review.append(row["text"])
      if int(row["stars"]) < 3:
        fsc_sentiment.append("0")
      else:
        fsc_sentiment.append("1")
      if count > 4000 :      #for testing we just choose the first 1500 records 
        break 

    for row in reader:
      count += 1 
      fsc_stars_test.append(row['stars'])
      fsc_review_test.append(row['text'])
      if int(row["stars"]) < 3:
        fsc_sentiment_test.append("0")
      else:
        fsc_sentiment_test.append("1")
      if count > 5200:
        break
#print to review the list data 
print(fsc_stars)
print(fsc_review)
print(fsc_sentiment)
print("-----------------")
print(fsc_stars_test)
print(fsc_review_test)
print(fsc_sentiment_test)

"""To better rearrange our dataset we use the pandas function "Dataframe""""

d_train = pd.DataFrame(data={'content': fsc_review, 'label': fsc_stars, 'sentiment': fsc_sentiment })
d_test = pd.DataFrame(data={'content': fsc_review_test, 'label': fsc_stars_test, 'sentiment': fsc_sentiment_test})
print(d_train)
print(d_test)
#print(len(d_train['label']))

plot = sns.barplot(y = d_train.label.value_counts(), x = d_train.label.value_counts().index)

"""# Data Cleaning

In order to yeald a more accurate  training, data must be cleaned and preprocessed appropriately. Therefore all elements which do not bear meaningful sentiment values (Punctuation, Stopwords, as well as low frequency words) are removed
"""

# Remove Punctionations from the text
def remove_punctuation(word):
    for punkt in list(string.punctuation):
        word = word.replace(punkt, '')
    return word

from sklearn.feature_extraction import text

stop_words = text.ENGLISH_STOP_WORDS

# Remove Stop Words form the text
def cleansing(text):
    text = text.lower()
    word_list = word_tokenize(text)
    word_list = [word for word in word_list if len(word) > 2] #words shorter than three characters are excluded from the dataset
    word_list = [remove_punctuation(word) for word in word_list]  #punctuation is removed
    text = ' '.join(word_list)
    fdist = FreqDist(text)
    low_freq = list(filter(lambda x: x[1]<10,fdist.items()))  
    text = [w for w in word_list if not w.lower() in stop_words and not w.lower() in low_freq]  #low frequency words are removed
    text = ' '.join(word_list)
    return text

d_train['content_cleansing'] = d_train.content.apply(cleansing)
print(d_train['content_cleansing'])
print(d_train['content'])

dist_plot = sns.displot(d_train.content_cleansing.apply(lambda x: len(x)))

"""Hereby a new column is created where the length of the related "content" field is displayed"""

d_train['cont_clean_len_token'] = d_train.content_cleansing.apply(lambda x: len(x))
print(d_train['cont_clean_len_token'])

"""# Stemming
By this operation words are brought to their basic form (lemmatization)
[deprecated]
"""

from nltk.stem import LancasterStemmer #or "PorterStemmer"

ps = LancasterStemmer()
texttostem = ' '.join([str(elem) for elem in d_train['content_cleansing']])
#texttostem = word_tokenize(texttostem)
for word in texttostem.split():
  word = ps.stem(word)
  #print(word)

print(texttostem)

# split the dataset into training and validation datasets 
texts_train = d_train['content_cleansing'].values
texts_test = d_test['content'].values
yy_train = pd.to_numeric(d_train['sentiment']).values
yy_test =  pd.to_numeric(d_test['sentiment']).values

"""#Count Vectors for Dataset

With the sklearn library we use a function (CountVectorizer) to create a type-dictionary of our dataset. Each word will be treated as a feature. This kind of processing is necessary to be able to assign a sentiment to each word.
"""

from sklearn.feature_extraction.text import CountVectorizer

train = ' '.join([str(elem) for elem in d_train['content_cleansing']])
#texttostem = word_tokenize(texttostem)
#train = ' '.join([str(elem) for elem in texttostem])
train = train.split()

count_vectorizer =  CountVectorizer(max_features = 854) #max_features

feature_vector = count_vectorizer.fit(train)

features = feature_vector.get_feature_names_out()

train_ds_features = count_vectorizer.transform(train)

features_counts =  np.sum(train_ds_features.toarray(),axis=0)

features_counts = pd.DataFrame(dict(features = features, counts = features_counts))

#print(len(feature_vector.vocabulary_))

features_counts.sort_values('counts', ascending=False)[0:100]

"""### Low frequency words identification (alternative to previous methods)



"""

features_counts = np.sum(train_ds_features.toarray(),axis=0)

features_counts_df = pd.DataFrame(dict(features = features, counts = features_counts))

plt.figure(figsize=(8,6))

plt.hist(features_counts_df.counts, bins=50, range=(0,2000))

plt.xlabel("Frequency of words")

plt.ylabel('Density')

len(features_counts_df[features_counts_df.counts==1])

"""From this graph we can notice that the amount of positive reviews is pretty higher than the amount of negative ones"""

sns.barplot(y = d_train.label.value_counts(), x = d_train.label.value_counts().index)

"""# Distribution of words across different sentiments



"""

#train = ' '.join([str(elem) for elem in d_train['content_cleansing']])
train_ds_df = pd.DataFrame(train_ds_features.todense()) 

train_ds_df.columns = features

train_ds_df['sentiment'] = d_train['sentiment']
train_ds_df['label'] = d_train['label']
sns.barplot(x = 'sentiment',y = 'best', data = train_ds_df, estimator= sum)  #the word whose occurrence across the sentiments we would
#like to explore is inserted in the second functionÂ´s parameter (y). Here we make the case for "best", which counterintuitively displays a couple of
#negative occurrences as well

"""
Other two examples showing labels instead of sentiments"""

sns.barplot(x = 'label',y = 'really', data = train_ds_df, estimator= sum)

sns.barplot(x = 'label',y = 'wife', data = train_ds_df, estimator= sum)

sns.barplot(x = 'sentiment',y = 'wife', data = train_ds_df, estimator= sum)

"""Another simple method to preprocess the dataset, is just taking in account very polarized reviews (1 or 5 stars). Thus the creation of the "Sentiment" column is not required"""

#Preparing for classification
f_class = f[(f['stars']==1) | (f['stars']==5)]
f_class.shape

#putting them in seperate variable
x = f_class['text']
y = f_class['stars']

"""Vectorization (as before, also simplified)"""

vectorizer = CountVectorizer(analyzer = cleansing).fit(texts_train)
print(len(vectorizer.vocabulary_))

review_0 = texts_train[0]
print(review_0)
vocab_0 = vectorizer.transform([review_0])
print(vocab_0)

print(vectorizer.get_feature_names()[23])
print(vectorizer.get_feature_names()[27])

#Now applying vectorization to the full review set which would check the shape of new x
x = vectorizer.transform(x)
print('Shape of Sparse Matrix: ', x.shape)
print('Amount of Non-Zero occurrences: ', x.nnz)

# Percentage of non-zero values
density = (100.0 * x.nnz / (x.shape[0] * x.shape[1]))
print("Density = ",density)

"""Splitting the transformed dataset into training data and test data in the proportion of 80:20 using the sklearn package train_test_split.
The splitting generally takes place at this point: after the preprocessing, before the training phase. Nonetheless, the previous dataset splitting method (and moment) is kept for pedagogical purposes 
"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x, y,test_size=0.2,random_state=101)
print(x_train.shape)
print(x_train)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

"""# Finally the training process can start. We are exploring  different models, so that the explanation can be performed on the one evaluated as most precise.

# **Multinomial Naive Bayes**

Based on the Bayes formula for conditional probability: P(A|B) = P(A) * P(B|A)/P(B)
Where we are calculating the probability of class A when predictor B is already provided.

P(B) = prior probability of B

P(A) = prior probability of class A

P(B|A) = occurrence of predictor B given class A probability

This formula helps in calculating the probability of the tags in the text.
"""

#Building the model
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(x_train, y_train)

#Testing our model
nb_predict = nb.predict(x_test)

#Creating the confusion matrix
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test, nb_predict))

print('\n')

#Creating the classification report
print(classification_report(y_test, nb_predict))  ### The model achieved 93% accuracy. 
                                                  ### However, since we know that there are some bias values, 
                                                  ### so let's just test it on a single review.

#positive single review
pos_review = f_class['text'][59]
pos_review

pos_review_t = vectorizer.transform([pos_review])
nb.predict(pos_review_t)[0]                       ### 5 star rating which is good as expected 


#Negative single review
neg_review = f_class['text'][281]
neg_review

neg_review_t = vectorizer.transform([neg_review])
nb.predict(neg_review_t)[0]                       ### 1 star rating which is fine as expected

"""# **K-NN (K-Nearest Neighbour) classifier**

K-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data.
It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.
KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.
"""

#Building the model
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)

#Testing our model on x_test
knn_predict = knn.predict(x_test)

#Creating the confusion matrix
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test, knn_predict))

print('\n')

#Creating the classification report
print(classification_report(y_test, knn_predict))

"""# **Support Vector Machine**

Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges.In the SVM algorithm, each data item is plotted as a point in n-dimensional space (where n is a number of features you have) with the value of each feature being the value of a particular coordinate. The classification is then performed by finding the hyper-plane that best differentiates the two classes
"""

#Building the model
from sklearn.svm import SVC
svm = SVC()
svm.fit(x_train, y_train)

#Testing our model on x_test
svm_predict = svm.predict(x_test)

#Creating the confusion matrix
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test, svm_predict))

print('\n')

#Creating the classification report
print(classification_report(y_test, svm_predict))

"""# **Random Forest Classifier**

A random forest is a supervised machine learning algorithm that is constructed from decision tree algorithms. The (random forest) algorithm establishes the outcome based on the predictions of the decision trees. It predicts by taking the average or mean of the output from various trees. Increasing the number of trees increases the precision of the outcome.
"""

#Building the model
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(x_train, y_train)

#Testing our model on x_test
rf_predict = rf.predict(x_test)

#Creating the confusion matrix
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test, rf_predict))

print('\n')

#Creating the classification report
print(classification_report(y_test, rf_predict))

"""
# **Bi-GRU-LSTM-CNN**
Bi-directional Gated Recurrent Unit Long Short Term Memory Convolutional Neural Networks

"""

from sklearn.base import BaseEstimator, TransformerMixin
from keras.models import Model, Input
from keras.layers import Dense, LSTM, Dropout, Embedding, SpatialDropout1D, Bidirectional, concatenate, Flatten, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, Conv1D, GRU
from keras.utils.vis_utils import plot_model
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import accuracy_score
from keras import backend as K 

#K.clear_session()

class KerasTextClassifier(BaseEstimator, TransformerMixin):
    'Wrapper class for keras text classification models that takes raw text as input.'
    def __init__(self, max_words=10000, input_length=100, emb_dim=20, n_classes=3, epochs=5, batch_size=32):
        self.max_words = max_words
        self.input_length = input_length
        self.emb_dim = emb_dim
        self.n_classes = n_classes
        self.epochs = epochs
        self.bs = batch_size
        self.classes_ = None
        self.model = self._get_model()
        self.tokenizer = Tokenizer(num_words=self.max_words+1,
                                   lower=True, split=' ', oov_token="UNK")
    
    def _get_model(self):
        input_text = Input((self.input_length,))
        text_embedding = Embedding(input_dim=self.max_words + 2, output_dim=self.emb_dim,
                                   input_length=self.input_length, mask_zero=False)(input_text)
        text_embedding = SpatialDropout1D(0.5)(text_embedding)
        x = Bidirectional(GRU(units=32, return_sequences = True))(text_embedding)
        x = Conv1D(int(32/2), kernel_size = 2, padding = "valid", kernel_initializer = "he_uniform")(x)

        y = Bidirectional(LSTM(units=32, return_sequences=True))(text_embedding)
        y = Conv1D(int(32/2), kernel_size = 2, padding = "valid", kernel_initializer = "he_uniform")(y)
        

        avg_pool1 = GlobalAveragePooling1D()(x)
        max_pool1 = GlobalMaxPooling1D()(x)
    
        avg_pool2 = GlobalAveragePooling1D()(y)
        max_pool2 = GlobalMaxPooling1D()(y)

        x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])

        x = Dense(500, activation="relu")(x)
        x = Dropout(0.5)(x)
        out = Dense(units=self.n_classes, activation="softmax")(x)
        model = Model(input_text, out)
        model.compile(optimizer="adam",
                      loss="sparse_categorical_crossentropy",
                      metrics=["accuracy"])
        plot_model(model, to_file='model_plot2.png',show_shapes=True, show_layer_names=True)
        return model
    
    def _get_sequences(self, texts):
        seqs = self.tokenizer.texts_to_sequences(texts)
        return pad_sequences(seqs, maxlen=self.input_length, value=0)
    
    def _preprocess(self, texts):
        return [re.sub("[^a-zA-Z]", " ", texts) for x in texts]
    
                          
    def fit(self, X, y):
        '''
        Fit the vocabulary and the model.
        
        :params:
        X: list of texts.
        y: labels.
        '''
        
        self.tokenizer.fit_on_texts(self._preprocess(X))
        self.tokenizer.word_index = {e: i for e,i in self.tokenizer.word_index.items() if i <= self.max_words}
        self.tokenizer.word_index[self.tokenizer.oov_token] = self.max_words + 1
        seqs = self._get_sequences(self._preprocess(X))
        self.model.fit(seqs, y, batch_size=self.bs, epochs=self.epochs, validation_split=0.1)
    
    def predict_proba(self, X, y=None):
        seqs = self._get_sequences(self._preprocess(X))
        return self.model.predict(seqs)
    
    def predict(self, X, y=None):
        return np.argmax(self.predict_proba(X), axis=1)
    
    def score(self, X, y):
        y_pred = self.predict(X)
        return accuracy_score(y, y_pred)
    
    def trasnform(self, X):
        return self._get_sequences(self._preprocess(X))

    def _encode_label(self, y):
        return self.label_encoder.transform(y)
        
    def _decode_label(self, y):
        return self.label_encoder.inverse_transform(y)

    def _init_label_encoder(self, y):
        self.label_encoder = LabelEncoder()
        self.label_encoder.fit(y)
        self.classes_ = self.label_encoder.classes_

    def _encode_feature(self, x):
        self.tokenizer.fit_on_texts(self._preprocess(x))
        self.tokenizer.word_index = {e: i for e,i in self.tokenizer.word_index.items() if i <= self.word_cnt}
        self.tokenizer.word_index[self.tokenizer.oov_token] = self.word_cnt + 1
        return self._get_sequences(self._preprocess(x))

    def predict_decode(self, X, y=None):
        return self._decode_label(self.predict(X))
        
    def summary(self):
       return self.model.summary()

text_model = KerasTextClassifier(epochs=4, max_words=10000, input_length=200)

text_model.summary()

train = ' '.join([str(elem) for elem in d_train['content_cleansing']])[0:4001]
history = text_model.fit(train, yy_train)

test = ' '.join([str(elem) for elem in d_test['content']])[0:1200]
text_model.score(test, yy_test)

"""# **Bi-LSTM MODEL**

Bi-directional long short term memory (Bi-LSTM) is a kind of LSTM model which has the ability to process the data in both forward and backward directions. Bi LSTM a sequence processing model consists of two LSTMs: one takes the input in forward direction, and the other in backwards direction. This model has shown to deliver better results according to accuracy, for which reason will be used in the explanation section as well.
"""

from sklearn.base import BaseEstimator, TransformerMixin #BaseEstimator just gives it the get_params and set_params methods that all Scikit-learn estimators
from keras.models import Model, Input #Keras is used for creating deep models 
#Dense  layer is the regular deeply connected neural network layer.
#It is most common and frequently used layer.
#Dense layer does the below operation on the input and return the output.
#Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time and SpatialDropout1D is subversion
#Embedding layer enables us to convert each word into a fixed length vector of defined size
#Layer that concatenates a list of inputs
#Flatten is used to flatten the input
from keras.layers import Dense, Embedding, Bidirectional, concatenate, Flatten 
from keras.layers import LSTM , Dropout , SpatialDropout1D
from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D
from keras.utils.vis_utils import plot_model #Converts a Keras model to dot format and save to a file
from keras.preprocessing.text import Tokenizer # The Tokenizer class of Keras is used for vectorizing a text corpus
from keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import accuracy_score
import regex as re
import numpy as np
# Wrapper class from keras text-classification models that takes raw text as input.
class KerasTextClassifier(BaseEstimator, TransformerMixin):    
    
    def __init__(self, max_words=30000, input_length=100, emb_dim=20, n_classes=3, epochs=5, batch_size=32):
        self.max_words = max_words
        self.input_length = input_length
        self.emb_dim = emb_dim
        self.n_classes = n_classes
        self.epochs = epochs
        self.bs = batch_size
        self.classes_ = None
        self.model = self._get_model()
        self.tokenizer = Tokenizer(num_words=self.max_words+1,
                                   lower=True, split=' ', oov_token="UNK")
    
    def _get_model(self):
        input_text = Input((self.input_length,))
        text_embedding = Embedding(input_dim=self.max_words + 2, output_dim=self.emb_dim,
                                   input_length=self.input_length, mask_zero=False)(input_text)
        text_embedding = SpatialDropout1D(0.5)(text_embedding)
        bilstm = Bidirectional(LSTM(units=32, return_sequences=True, recurrent_dropout=0.5))(text_embedding)
        # x = concatenate([GlobalAveragePooling1D()(bilstm), GlobalMaxPooling1D()(bilstm)])
        flatten_layer = Flatten()
        x= flatten_layer(bilstm)
        # Dense layer with "ReLU" activation
        x = Dense(512, activation="relu")(x)
        # Dropout layer
        x = Dropout(0.5)(x)
        # Dense layer with "Softmax" activation
        out = Dense(units=self.n_classes, activation="softmax")(x)
        model = Model(input_text, out)

        # Specifying the training configurations (optimizer, loss function, accuracy metrics)
        model.compile(optimizer="adam", # Optimizer
                      # Loss function to minimize
                      loss="sparse_categorical_crossentropy", 
                      # Accuracy Metrics to monitor
                      metrics=["accuracy"])
        plot_model(model, to_file='model_plot.png',show_shapes=True, show_layer_names=True)
        return model
    
    def _get_sequences(self, texts):
        seqs = self.tokenizer.texts_to_sequences(texts)
        return pad_sequences(seqs, maxlen=self.input_length, value=0)
    
    def _preprocess(self, texts):
        return [re.sub(r"\d", "DIGIT", x) for x in texts]
    
    def fit(self, X, y):
        '''
        Fit the vocabulary and the model.
        
        :params:
        X: list of texts.
        y: labels.
        '''
        
        self.tokenizer.fit_on_texts(self._preprocess(X))
        self.tokenizer.word_index = {e: i for e,i in self.tokenizer.word_index.items() if i <= self.max_words}
        self.tokenizer.word_index[self.tokenizer.oov_token] = self.max_words + 1
        seqs = self._get_sequences(self._preprocess(X))
        self.model.fit(seqs, y, batch_size=self.bs, epochs=self.epochs, validation_split=0.1)
    
    def predict_proba(self, X, y=None):
        seqs = self._get_sequences(self._preprocess(X))
        return self.model.predict(seqs)
    
    def predict(self, X, y=None):
        return np.argmax(self.predict_proba(X), axis=1)
    
    def score(self, X, y):
        y_pred = self.predict(X)
        return accuracy_score(y, y_pred)
    
    def trasnform(self, X):
        return self._get_sequences(self._preprocess(X))

    def _encode_label(self, y):
        return self.label_encoder.transform(y)
        
    def _decode_label(self, y):
        return self.label_encoder.inverse_transform(y)

    def _init_label_encoder(self, y):
        self.label_encoder = LabelEncoder()
        self.label_encoder.fit(y)
        self.classes_ = self.label_encoder.classes_

    def _encode_feature(self, x):
        self.tokenizer.fit_on_texts(self._preprocess(x))
        self.tokenizer.word_index = {e: i for e,i in self.tokenizer.word_index.items() if i <= self.word_cnt}
        self.tokenizer.word_index[self.tokenizer.oov_token] = self.word_cnt + 1
        return self._get_sequences(self._preprocess(x))

    def predict_decode(self, X, y=None):
        return self._decode_label(self.predict(X))
        
    def summary(self):
       return self.model.summary()
text_model2 = KerasTextClassifier(epochs=2, max_words=20000, input_length=200)

text_model2.summary()

from IPython.display import Image
Image(filename='model_plot.png')

history = text_model2.fit(texts_train, yy_train)

text_model2.score(texts_test, yy_test)

"""In the following section the explanation methods SHAP, LIME and ELI5 are used.

## **Explaining with SHAP Explaination (kernel Explaination)**

SHAP is a viable library which helps to make pretty visualizations of simple numeric measures to see which features mattered to a model. This helps to make comparisons between features easily, and one can present the resulting graphs to non-technical audiences to explain the internal mechanism of the black box models.
"""

encoded_x_train = text_model2.trasnform(texts_train)
encoded_x_test = text_model2.trasnform(texts_test)

# Generate the description of Integer numbers i.e 50 to Fifty
def prepare_explanation_words(pipeline, encoded_x):
    words = pipeline.tokenizer.word_index
    num2word = {}
    for w in words.keys():
        num2word[words[w]] = w
    x_test_words = np.stack([
        np.array(list(map(
            lambda x: num2word.get(x, "NONE"), encoded_x[i])
                     )) for i in range(10)])

    return x_test_words

import shap

# Initialize SHAP Js to display explaination visualizations
shap.initjs()

# Defining the SHAP explainer
kernel_explainer = shap.KernelExplainer(text_model2.model.predict, encoded_x_train[:10])

# Explain the sentiments of the pipeline on the first 10 samples
kernel_shap_values = kernel_explainer.shap_values(encoded_x_test[:10])

x_test_words = prepare_explanation_words(text_model, encoded_x_test)
y_pred = text_model2.predict(texts_test[:10])

"""SHAP Explanation visualization for different dataset rows"""

# Initialize SHAP Js to display explaination visualizations
shap.initjs()
print('Actual Category: %s, Predict Category: %s' % (y_test[6], y_pred[6]))

# Display explanation visualization
shap.force_plot(kernel_explainer.expected_value[0], kernel_shap_values[0][6], x_test_words[6])

# Initialize SHAP Js to display explaination visualizations
shap.initjs()
print('Actual Category: %s, Predict Category: %s' % (yy_test[9], y_pred[9]))

# Display explanation visualization
shap.force_plot(kernel_explainer.expected_value[0], kernel_shap_values[0][9], x_test_words[9])

# Initialize SHAP Js to display explaination visualizations
shap.initjs()
print('Actual Category: %s, Predict Category: %s' % (yy_test[8], y_pred[8]))

# Display explanation visualization
shap.force_plot(kernel_explainer.expected_value[0], kernel_shap_values[0][8], x_test_words[8])

# Initialize SHAP Js to display explaination visualizations
shap.initjs()

# Display interactive explanation visualization
shap.force_plot(kernel_explainer.expected_value[0], kernel_shap_values[0][:1000:,], x_test_words[6])

# Initialize SHAP Js to display explaination visualizations
shap.initjs()

# Display SHAP Summary Plots to highlight feature importance
shap.summary_plot(kernel_shap_values,features=x_test_words )
shap.summary_plot(kernel_shap_values[1],features=x_test_words ,max_display=10 )

"""SHAP summary plots give us a birds-eye view of feature importance and what is driving it. The above shown plot is made of many dots. Each dot has three characteristics:


*   Vertical location shows what feature it is depicting
*   Color shows whether that feature was high or low for that row of the dataset
*   Horizontal location shows whether the effect of that value caused a higher or lower prediction
"""

# Initialize SHAP Js to display explaination visualizations
shap.initjs()
print('Actual Category: %s, Predict Category: %s' % (yy_test[9], y_pred[9]))

shap.force_plot(kernel_explainer.expected_value[0], kernel_shap_values[0][9], x_test_words[9])

# Initialize SHAP Js to display explaination visualizations
shap.initjs()
print('Actual Category: %s, Predict Category: %s' % (yy_test[8], y_pred[8]))

shap.force_plot(kernel_explainer.expected_value[0], kernel_shap_values[0][8], x_test_words[8])

"""### **Explain the Visualization**

*   This force plot of Kernel Explanation explains how different features pushed and pulled on the output to move it from the base_value to the model output value or prediction. The prediction of this kernel explanation is the probability or confidence that the review is 100% belong to predicted category  ("Negative" Class).

##**Explaining with Lime**
"""

from collections import OrderedDict
from lime.lime_text import LimeTextExplainer

import seaborn as sns
sns.despine() #removes the spines from the right and upper portion in plot
# We choose a sample from test set
#text sample from test set
idx =6
idx2 =9
idx3=8
text_sample = texts_test[idx]
text_sample2 = texts_test[idx2]
text_sample3 = texts_test[idx3]
class_names = ['Negative', 'Positive']

print('Sample {}: last 80 words (only part used by the model)'.format(idx))
print('-'*50)
print(" ".join(text_sample.split()[-80:]))
print('-'*50)
print('Probability(positive) =', text_model2.predict_proba([text_sample])[0,1])
print("True class: ",y_test[idx]) #print in actual classes

explainer = LimeTextExplainer(class_names=class_names)
explanation = explainer.explain_instance(text_sample, text_model2.predict_proba, num_features=10) #Passing sample text and prediction and feature to LIME explainer 

weights = OrderedDict(explanation.as_list())
lime_weights = pd.DataFrame({'words': list(weights.keys()), 'weights': list(weights.values())})

sns.barplot(x="words", y="weights", data=lime_weights);
plt.xticks(rotation=45)
plt.title('Sample {} features weights given by LIME'.format(idx));

explanation.show_in_notebook(text=True)

#applying sample idx2
print('Sample {}: last 80 words (only part used by the model)'.format(idx2))
print('-'*50)
print(" ".join(text_sample.split()[-80:]))
print('-'*50)
print('Probability(positive) =', text_model2.predict_proba([text_sample])[0,1])
print("True class: ",yy_test[idx2])

explainer = LimeTextExplainer(class_names=class_names)
explanation = explainer.explain_instance(text_sample2, text_model2.predict_proba, num_features=10)

weights = OrderedDict(explanation.as_list())
lime_weights = pd.DataFrame({'words': list(weights.keys()), 'weights': list(weights.values())})



sns.barplot(x="words", y="weights", data=lime_weights);
plt.xticks(rotation=45)
plt.title('Sample {} features weights given by LIME'.format(idx2));

explanation.show_in_notebook(text=True)

#Applying the sample text idx3
print('Sample {}: last 80 words (only part used by the model)'.format(idx3))
print('-'*50)
print(" ".join(text_sample.split()[-80:]))
print('-'*50)
print('Probability(positive) =', text_model2.predict_proba([text_sample])[0,1])
print("True class: ",yy_test[idx3])

explainer = LimeTextExplainer(class_names=class_names)
explanation = explainer.explain_instance(text_sample3, text_model2.predict_proba, num_features=10)

weights = OrderedDict(explanation.as_list())
lime_weights = pd.DataFrame({'words': list(weights.keys()), 'weights': list(weights.values())})



sns.barplot(x="words", y="weights", data=lime_weights);
plt.xticks(rotation=45)
plt.title('Sample {} features weights given by LIME'.format(idx3));

explanation.show_in_notebook(text=True)

"""### **Explain the Visualization**

*   On the "Text with higligted words", the blue words made the classifier think that the review was more negative while the yellow words made the review seem more positive.

*   The explainer predicts that this review should be labeled as "positive" with the probability of above 90%.


With LIME methods of explanation, it is easy for Human to intrepret and understand the prediction. LIME allows us to extract some important word that mostly contribute to the prediction, and even give the rational number "Negative" or "Positive" contribution of each word of sample.

# **ELI5 explainable model**
"""

from sklearn.base import BaseEstimator, TransformerMixin
from keras.models import Model, Input
#Keras is used for creating deep models 
#Dense  layer is the regular deeply connected neural network layer.
#It is most common and frequently used layer.
#Dense layer does the below operation on the input and return the output.
#Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time and SpatialDropout1D is subversion
#Embedding layer enables us to convert each word into a fixed length vector of defined size
#Layer that concatenates a list of inputs
from keras.layers import Dense, LSTM, Dropout, Embedding, SpatialDropout1D, Bidirectional, concatenate
from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import accuracy_score
from eli5.lime import TextExplainer
import regex as re # Regular Expression, is a sequence of characters that forms a search pattern
import numpy as np #NumPy is a Py lib used for working with arrays and provide fundamental packages for scientific computing

class KerasTextClassifier(BaseEstimator, TransformerMixin):
    '''Wrapper class for keras text classification models that takes raw text as input.'''
    
    def __init__(self, max_words=30000, input_length=100, emb_dim=20, n_classes=4, epochs=5, batch_size=32):
        self.max_words = max_words
        self.input_length = input_length
        self.emb_dim = emb_dim
        self.n_classes = n_classes
        self.epochs = epochs
        self.bs = batch_size
        self.model = self._get_model()
        self.tokenizer = Tokenizer(num_words=self.max_words+1,
                                   lower=True, split=' ', oov_token="UNK")
    
    def _get_model(self):
        input_text = Input((self.input_length,))
        text_embedding = Embedding(input_dim=self.max_words + 2, output_dim=self.emb_dim,
                                   input_length=self.input_length, mask_zero=False)(input_text)
        text_embedding = SpatialDropout1D(0.5)(text_embedding)
        bilstm = Bidirectional(LSTM(units=32, return_sequences=True, recurrent_dropout=0.5))(text_embedding)
        x = concatenate([GlobalAveragePooling1D()(bilstm), GlobalMaxPooling1D()(bilstm)])
        x = Dropout(0.7)(x)
        x = Dense(512, activation="relu")(x)#ReLu is a non-linear activation function that is used in multi-layer neural networks or deep neural networks
        x = Dropout(0.6)(x)
        x = Dense(512, activation="relu")(x)
        x = Dropout(0.5)(x)
        out = Dense(units=self.n_classes, activation="softmax")(x)
        model = Model(input_text, out)
        model.compile(optimizer="adam",
                      loss="sparse_categorical_crossentropy",
                      metrics=["accuracy"])
        return model
    
    def _get_sequences(self, texts):
        seqs = self.tokenizer.texts_to_sequences(texts) #Transforms each text in texts to a sequence of integers
        return pad_sequences(seqs, maxlen=self.input_length, value=0)
    
    def _preprocess(self, texts):
        return [re.sub(r"\d", "DIGIT", x) for x in texts]
    
    def fit(self, X, y): #Fit the vocabulary and the model. :params: X: list of texts. y: labels.
     
        '''
        Fit the vocabulary and the model.
        
        :params:
        X: list of texts.
        y: labels.
        '''
        
        self.tokenizer.fit_on_texts(self._preprocess(X))
        self.tokenizer.word_index = {e: i for e,i in self.tokenizer.word_index.items() if i <= self.max_words}
        self.tokenizer.word_index[self.tokenizer.oov_token] = self.max_words + 1
        seqs = self._get_sequences(self._preprocess(X))
        self.model.fit(seqs, y, batch_size=self.bs, epochs=self.epochs, validation_split=0.1)
    
    def predict_proba(self, X, y=None):
        seqs = self._get_sequences(self._preprocess(X))
        return self.model.predict(seqs)
    
    def predict(self, X, y=None):
        return np.argmax(self.predict_proba(X), axis=1)
    
    def score(self, X, y): #return prediction score accuracy
        y_pred = self.predict(X)
        return accuracy_score(y, y_pred)

doc = texts_test[6]
te = TextExplainer(random_state=42)
te.fit(doc, text_model2.predict_proba)
te.show_prediction(target_names=yy_train.all())